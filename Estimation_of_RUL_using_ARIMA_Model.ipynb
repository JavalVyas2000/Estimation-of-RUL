{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Estimation of RUL using ARIMA Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1cLune3kSPCWcQwtzjFRQ1j8w7Syh05Fv",
      "authorship_tag": "ABX9TyNT0f0bpeILDhzGUPjc6Xd9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JavalVyas2000/Estimation-of-RUL/blob/main/Estimation_of_RUL_using_ARIMA_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RcjxXMOgacFD"
      },
      "outputs": [],
      "source": [
        "# installing all the prerequisited for connecting with the drive.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "p_h1r5Wwdi_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing ARIMA \n",
        "!pip3 install pmdarima"
      ],
      "metadata": {
        "id": "qH3JqATjq3-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the python libraries that we may use\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "# from datetime import datetime, date, timedelta\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # advanced vizs\n",
        "%matplotlib inline\n",
        "\n",
        "# statistics\n",
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "\n",
        "# time series analysis\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "import statsmodels.tools.eval_measures as  em\n",
        "from sklearn.metrics import  mean_squared_error\n",
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "from IPython.display import display\n",
        "from pylab import rcParams\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from datetime import date"
      ],
      "metadata": {
        "id": "owP5vUZPq4zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing openpyxl to adjust to the version\n",
        "!pip install openpyxl==3.0.0"
      ],
      "metadata": {
        "id": "JoL-VGCSrFM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data\n",
        "data1=pd.read_excel('gdrive/MyDrive/RUL data/Discap_Calculated.xlsx')\n",
        "valide=pd.read_excel('gdrive/MyDrive/RUL data/Discap_Calculated.xlsx')"
      ],
      "metadata": {
        "id": "8sW6ooYdrISl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_EinmWjyrMOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning the data and making the cycles continuous\n",
        "\n",
        "data1['Cycle']=data1['Cycle']//2\n",
        "cycle_no=[]\n",
        "p=data1['Cycle'][data1.shape[0]-1]\n",
        "for i in range(p):\n",
        "  cycle_no.append(i+1)\n",
        "Discap=[None]*len(cycle_no)\n",
        "for i in range(data1.shape[0]-1):\n",
        "  Discap[data1['Cycle'][i]]=data1['Discap'][i]"
      ],
      "metadata": {
        "id": "kVUnwl7lrNGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe and interpolating the missing values\n",
        "df1 = pd.DataFrame(Discap,cycle_no)\n",
        "df1.interpolate(method ='linear', limit_direction ='forward',axis=0,inplace=True)\n",
        "df1.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "D41nU3E-rQ_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new dataframe that is differenced once. \n",
        "df1_dif=df1.diff(1)\n",
        "df1_dif.dropna(inplace=True)\n",
        "plt.plot(df1_dif)"
      ],
      "metadata": {
        "id": "us3kYS2QrTYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dividing the dataframe into train and test modes. \n",
        "size=72  # Size of training data set is either 72 or 120 based on the https://www.mdpi.com/1996-1073/14/21/7206 research paper. \n",
        "df11_dif=df1_dif[0:size] \n",
        "test1= df1_dif[size:]\n",
        "df11_dif.plot(figsize=(12,5))"
      ],
      "metadata": {
        "id": "-v0paTmiszBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pmdarima as pm\n",
        "from pmdarima.arima import auto_arima\n",
        "import statsmodels.tsa.arima.model as arima"
      ],
      "metadata": {
        "id": "UOjNIz4eu9gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the arima model based on the parameters found analytically\n",
        "model = arima.ARIMA(df11_dif, order = (1, 0, 60)) \n",
        "\n",
        "result = model.fit() \n",
        "result.summary() \n",
        "\n",
        "# start the predictions from the last cycle of the training dataset to the end of test\n",
        "start_index = len(df11_dif)\n",
        "end_index = len(df1)\n",
        "prediction = result.predict(start_index, end_index) \n",
        "print(prediction)\n",
        "\n",
        "# plot predictions and actual values \n",
        "plt.plot(prediction,label='Prediction')\n",
        "plt.plot(test1,label='Test')\n",
        "plt.legend(loc='best')"
      ],
      "metadata": {
        "id": "Pc2DQf1uvKL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function to get the original values from the differenced values\n",
        "def inverse_difference(last_ob, value):\n",
        "\treturn value + last_ob\n",
        "\n",
        "# take the inverse of the differenced values to get the original predictions and compare it with the test\n",
        "predictions = [inverse_difference(df1.values[size+i], prediction.values[i]) for i in range(len(prediction)-1)]\n",
        "test1_inv=[inverse_difference(df1.values[size+i], test1.values[i]) for i in range(len(test1)-1)]\n",
        "\n",
        "# plot the results of the predictions and test\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(np.arange(size,size+len(predictions)),predictions,label='Prediction')\n",
        "plt.plot(np.arange(size,size+len(test1_inv)),test1_inv,label='Test')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('Cycle_no')\n",
        "plt.ylabel('Discap')\n",
        "plt.title('B0006')"
      ],
      "metadata": {
        "id": "NfLM8ZEevtA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics - RMSE, MSE, MAPE, MAE\n",
        "import math\n",
        "testScore1 = math.sqrt(mean_squared_error(test1_inv, predictions[:-2]))\n",
        "print('Test Score1: %.4f RMSE' % (testScore1))\n",
        "testScore11 = (mean_squared_error(test1_inv, predictions[:-2]))\n",
        "print('Test Score11: %.4f MSE' % (testScore11))\n",
        "pred=np.array(predictions[:-2])\n",
        "actual=np.array(test1_inv)\n",
        "MAPE1=np.mean(np.abs((actual-pred)/actual))*100\n",
        "print('Test Score: %.4f MAPE' % (MAPE1))\n",
        "MAE1=np.mean(np.abs((actual-pred)))\n",
        "print('Test Score: %.4f MAE' % (MAE1))"
      ],
      "metadata": {
        "id": "VNrdx2mswJkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}